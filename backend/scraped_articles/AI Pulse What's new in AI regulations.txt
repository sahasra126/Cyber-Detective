Title: AI Pulse: What's new in AI regulations?
URL: https://www.trendmicro.com/en_in/research/24/i/ai-regulations-2024.html
Published Date: 2024-09-30 00:00:00

California’s proposed SB 1047 bill to mitigate AI risks sparked a storm of controversy over the summer and raised questions that still need clear answers—from how to determine AI risk to the potential impacts on AI innovation. AI model development is far from the only domain where frameworks are needed to prevent unwanted harms. This edition of AI Pulse takes a look at the topic of AI regulation and touches as well on some related issues, including how AI should be used in war and what’s going to happen to AI development when models run out of data.

Love it or Hate it, AI Regulation is Here to Stay

Even as the EU, UK, and United States were signing the first-ever legally binding treaty on AI in September 2024, experts at the European Centre for Not-for-Profit Law were calling the agreement toothless—symptomatic of where we are with AI right now.

With “seize the future” champions lined up squarely against “the end is nigh” critics of AI’s breakneck evolution, legislators are in the awkward spot of having to propose regulatory guardrails with little consensus on how to evaluate risks, anticipate capabilities, or plan for the technology’s future trajectory.

This issue of AI Pulse looks at some of the challenges and thorny questions surrounding AI regulation along with the latest threat trends and the challenges ahead as AI companies run out of fresh data for training their models.

[AI News]

What’s New in AI Regulations

Out front – or out to lunch?

Hot on the heels of a star turn protecting performers from unlawful use of their digital likenesses, California lawmakers captured headlines and caught flak this summer for a more sweeping piece of proposed AI legislation, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047).

AI luminaries like Geoffrey Hinton and Yoshua Bengio applauded the bill, which is about to become law, while critics called the legislation off-base. Andrew Ng told Forbes that SB 1047 makes a “fundamental mistake” by regulating AI as a technology instead of focusing on specific applications. Others said it could stifle innovation.

That last worry isn’t wholly supported by real-world experience. Financial services and healthcare are famously regulated sectors and yet also leaders in AI adoption—demonstrated by recent Canadian research on AI's ability to reduce unexpected hospital deaths.

At the very least, California’s jump into the AI legislation deep end has helped highlight crucial questions other jurisdictions will need to answer if they want to follow suit—which is likely, since most Americans think AI companies should be held responsible if their technology does harm.

How many zeroes make a threat?

California decided its new law would apply to AI models that cost $100 million or more to build and are trained on at least 1026 floating-point operations (that’s one with an impressive 26 zeroes in tow). Pretty much everyone acknowledges those are imprecise measures of AI threat potential. But how should lawmakers and regulators determine AI risk? AI companies have their own frameworks, yet these also sometimes raise questions.

Case in point: OpenAI released a scorecard in September for its new o1 model. It ranked low-risk on autonomy and cybersecurity and medium-risk on persuasion and chemical, biological, radiological, and nuclear (CBRN) dangers. The company considers anything medium or lower to be deployable, though Yoshua Bengio told Newsweek that o1’s medium-risk CBRN score “...reinforces the importance and urgency to adopt legislation like SB 1047 in order to protect the public.” Persuasive behaviour may be even more of a concern than dangerous information. The deceptive capabilities in o1 have also increased, raising concerns of Rogue AI.

The best bet may be for industry and government to work together on AI safety. A positive step in that direction was the late-August agreement between the National Institute of Standards and Technology (NIST), OpenAI, and Anthropic to collaborate on AI safety research, testing and evaluation.